html_node(".tablesaw-sortable-btn")
html_table()
rm(weather_page)
weather_page <- read_html("https://www.wunderground.com/history/daily/us/ma/boston/KBOS/date/2018-3-1")
weather_page %>%
html_node(".tablesaw-sortable-btn , td:nth-child(2)")
html_table()
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(XML)
library(httr)
library(ggplot2)
getwd()
setwd("statcom")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(XML)
library(httr)
library(ggplot2)
Cars93
attach(Cars93)
hist(MPG.highway, xlab="MPG.highway",main = "otoyol")
hist(MPG.city, xlab="MPG.city",main = "sehir")
p1<- ggplot(Cars93, aes(x = MPG.highway)) + xlab("MPG Highway")
p1 + geom_histogram()
p2 <- ggplot(Cars93, aes(x = MPG.city)) + xlab("MPG City")
p2 + geom_histogram()
#The shape of highway and city variables are asymmetric,they include potential outliers resulting in heavy tails
p1 <- ggplot(Cars93, aes(x = MPG.highway)) + xlab("MPG Highway")
p1 + geom_density()
p2 <- ggplot(Cars93, aes(x = MPG.city)) + xlab("MPG City")
p2 + geom_density()
Potential.outliers.highway <- (subset(Cars93, colnames(data), subset = MPG.highway > 40))
Potential.outliers.city<- (subset(Cars93, colnames(data), subset = MPG.highway > 40))
#The tails include some values larger than 40.observations might be chategorized as potential outliers
dim(Cars93)
nonusa <- Cars93[Origin == "non-USA",c(1:27)]
usa <- Cars93[Origin == "USA",c(1:27)]
Potential.outliers.highway.usa <- (subset(usa, colnames(data), subset = MPG.highway > 40))
Potential.outliers.city.usa <- (subset(usa, colnames(data), subset = MPG.city > 40))
Potential.outliers.highway.nonusa<- (subset(nonusa, colnames(data), subset = MPG.highway > 40))
Potential.outliers.city.nonusa<- (subset(nonusa, colnames(data), subset = MPG.city > 40))
Cars93.log <- transform(Cars93, MPG.highway = log(MPG.highway), MPG.city = log(MPG.city))
p1 <- ggplot(Cars93.log, aes(x = MPG.highway)) + xlab("MPG Highway")
p1 + geom_density()
p2 <- ggplot(Cars93.log, aes(x = MPG.city)) + xlab("MPG City")
p2 + geom_density()
#The distributions appear less skewed than before.
table(DriveTrain, Origin)
#The table shows that there is no significant difference between USA and nonUSA made cars in terms of DriveTrain.
summary(Cars93)
with(Cars93, aggregate(Price, by = list(Origin, Manufacturer), FUN = summary))
with(Cars93, tapply(Price, INDEX = list(Origin, Manufacturer), FUN = summary))
summary(usa)
base.plot <- ggplot(usa, aes(x = MPG.highway)) + xlab("USAcars")
base.plot + geom_density()
summary(nonusa)
base.plot <- ggplot(nonusa, aes(x = MPG.highway)) + xlab("NONUSAcars")
base.plot + geom_density()
#Check MPG.highway and MPG.city for existence of any potential outliers conditioning on Origin using summary
#What would be your choice for central tendency and spread measures?
#As mean and median values are pretty close for MPG.highway and MPG.city,
#recommend using mean as the measure of central tendency.providing standard deviation the best strategy
isPassingGrade <- function (x) {
x>=50
}
Ali <- 90
isPassingGrade(Ali)
Ceyhun <- 30
isPassingGrade(Ceyhun)
sendMessage <- function(x){
if (isPassingGrade(x) == TRUE){
"Congratulations"
} else {"Oh no!"}
}
sendMessage(Ali)
sendMessage(Ceyhun)
gradeSummary <- function(x){
if (x <= 100 & x >= 90){
list(letter.grade = "A", passed = TRUE)}
else if (x < 90 & x >= 80){
list(letter.grade = "B", passed = TRUE)}
else if (x>=0 & x < 80){
list(letter.grade = "F", passed = FALSE)}
}
gradeSummary(Ali)
MissingValueReplacer <- function(x){
if(is.na(x) == FALSE){
mean(x, na.rm=TRUE)
}
}
mysummary <- function(x){
x <- MissingValueReplacer(x)
c(mean(x), median(x), sd(x))
}
#blue.plot <- function(x, y, ...) {   par(mfrow=c(2,2))   plot(x, col="blue", ...)   plot(y, col="blue", ...)   plot(x, y, col="blue", ...) } blue.plot(Cars93$MPG.highway,Cars93$MPG.city)
knitr::opts_chunk$set(echo = TRUE)
```# Create a function to print squares of numbers in sequence.
attach(faithful)
attach(faithful)
#Apply the simple linear regression model for the data set faithful,
#and estimate the next eruption duration if the waiting time since the last eruption has been 80 minutes.
#We apply the lm function to a formula that describes the variable eruptions by the variable waiting, and save the linear regression model in a new variable.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
View(eruption.lm)
#Then we extract the parameters of the estimated regression equation with the coefficients function.
coeffs = coefficients(eruption.lm); coeffs
#We now fit the eruption duration using the estimated regression equation.
waiting = 80
duration = coeffs[1] + coeffs[2]*waiting #y^=a+bx
duration
#SOL2
#We wrap the waiting parameter value inside a new data frame named newdata.
newdata = data.frame(waiting=80)
#Then we apply the predict function to eruption.lm along with newdata.
predict(eruption.lm, newdata)
#Find the coefficient of determination for the simple linear regression model of the data set faithful.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#Then we extract the coefficient of determination from the r.squared attribute of its summary.
summary(eruption.lm)$r.squared
#Decide whether there is a significant relationship between the variables in the linear regression model of the data set faithful at .05 significance level.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
summary(eruption.lm)
#faithful, develop a 95% confidence interval of the mean eruption duration for the waiting time of 80 minutes.
newdata = data.frame(waiting=80)
#apply the predict function and set the predictor variable in the newdata argument.
#also set the interval type as "confidence", and use the "default 0.95 confidence level".
predict(eruption.lm, newdata, interval="confidence")
f(80)
f <- function(x) {
newdata = data.frame(waiting=x)
predict.lm = predict(eruption.lm, newdata, interval="confidence")
return(predict.lm)
}
f(80)
x = 10:100
f(x)
predict = f(x)
plot(eruptions~waiting, data=faithful)
lines(x,predict[,1],col="red")
lines(x,predict[,2],col="blue")
lines(x,predict[,3],col="blue")
#faithful, develop a 95% prediction interval of the eruption duration for the waiting time of 80 minutes
eruption.lm = lm(eruptions ~ waiting)
#faithful, develop a 95% prediction interval of the eruption duration for the waiting time of 80 minutes
eruption.lm = lm(eruptions ~ waiting)
newdata = data.frame(waiting=80)
#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval type as "predict", and use the "default 0.95 confidence level".
predict(eruption.lm, newdata, interval="predict")
f1 <- function(x) {
newdata = data.frame(waiting=x)
predict.lm = predict(eruption.lm, newdata, interval="predict")
return(predict.lm)
}
f1(80)
x = 10:100
f1(x)
predict1 = f1(x)
plot(eruptions~waiting, data=faithful)
lines(x,predict1[,1],col="red")
lines(x,predict1[,2],col="blue")
lines(x,predict1[,3],col="blue")
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.res = resid(eruption.lm)
plot(faithful$waiting, eruption.res,
ylab="Residuals", xlab="Waiting Time",
main="Old Faithful Eruptions")
abline(0, 0)# the horizon
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.stdres = rstandard(eruption.lm)
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.stdres = rstandard(eruption.lm)
plot(faithful$waiting, eruption.stdres,
ylab="Standardized Residuals",
xlab="Waiting Time",
main="Old Faithful Eruptions")
abline(0, 0)# the horizon
#Create the normal probability plot for the standardized residual of the data set faithful.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#compute the standardized residual with the rstandard function
eruption.stdres = rstandard(eruption.lm)
qqline(eruption.stdres)
qqnorm(eruption.stdres,
ylab="Standardized Residuals",
xlab="Normal Scores",
main="Old Faithful Eruptions")
qqline(eruption.stdres)
install.packages("lmodel2")
library("lmodel2", lib.loc="~/R/win-library/3.5")
## Example 1 (surgical unit data)
data(mod2ex1)
View(mod2ex1)
Ex1.res <- lmodel2(Predicted_by_model ~ Survival, data=mod2ex1, nperm=99)
Ex1.res > plot(Ex1.res)
## Example 2 (eagle rays and Macomona)
data(mod2ex2)
View(mod2ex2)
Ex2.res <- lmodel2(Prey ~ Predators, data=mod2ex2, "relative", "relative", 99)
Ex2.res
op <- par(mfrow = c(1,2))
plot(Ex2.res, "SMA")
plot(Ex2.res, "RMA")
par(op)
## Example 3 (cabezon spawning)
data(mod2ex3)
op <- par(mfrow = c(1,2))
View(mod2ex3)
Ex3.res <- lmodel2(No_eggs ~ Mass, data=mod2ex3, "relative", "relative", 99 )
Ex3.res > plot(Ex3.res, "SMA") > plot(Ex3.res, "RMA")
plot(Ex3.res, "SMA") > plot(Ex3.res, "RMA")
par(op)
plot(Ex3.res, "SMA")
plot(Ex3.res, "RMA")
par(op)
## Example 4 (highly correlated random variables)
data(mod2ex4)
op <- par(mfrow=c(1,2))
Ex4.res <- lmodel2(y ~ x, data=mod2ex4, "interval", "interval", 99)
Ex4.res > plot(Ex4.res, "OLS")
plot(Ex4.res, "OLS")
plot(Ex4.res, "MA")
par(op)
plot(Ex1.res)
## Example 5 (uncorrelated random variables)
data(mod2ex5)
op <- par(mfrow = c(2,2))
Ex5.res <- lmodel2(random_y ~ random_x, data=mod2ex5, "interval", "interval", 99)
plot(Ex5.res, "OLS")
plot(Ex5.res, "MA")
plot(Ex5.res, "SMA")
par(op)
## Example 6 where cor(y,x) = 0 by construct (square grid of points)
y0 = rep(c(1,2,3,4,5),5)
view(y0)
View(y0)
x0 = c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5))
View(x0)
plot(x0, y0)
zero.res = lmodel2(y0 ~ x0, data=Ex6, "relative", "relative")
Ex6 = as.data.frame(cbind(x0,y0))
zero.res = lmodel2(y0 ~ x0, data=Ex6, "relative", "relative")
print(zero.res)
op <- par(mfrow = c(1,2))
plot(zero.res, "OLS")
plot(zero.res, "MA")
par(op)
t.test(x, alternative = "less", mu = 10)
t.test(Ex6, alternative = "less", mu = 10)
#Is there evidence that the "mean level" of Salmonella in the ice cream is "greater than 0.3 MPN/g?"
#H0:  mu = 0.3  Ha:  mu > 0.3
x = c(0.593, 0.142, 0.329, 0.691, 0.231, 0.793, 0.519, 0.392, 0.418)
t.test(x, alternative="greater", mu=0.3)
Control = c(91, 87, 99, 77, 88, 91)
Treat = c(101, 110, 103, 93, 99, 104)
t.test(Control,Treat,alternative="less", var.equal=TRUE) # assuming equal standard deviation
t.test(Control,Treat,alternative="less") #not assuming equal standard deviation
reg = c(16, 20, 21, 22, 23, 22, 27, 25, 27, 28)
prem = c(19, 22, 24, 24, 25, 25, 26, 26, 28, 32)
t.test(prem,reg,alternative="greater", paired=TRUE)
attach(InsectSprays)
str(InsectSprays)
tapply(count, spray, mean)
tapply((count, spray,var)
tapply(count, spray,var)
tapply(count, spray, length)
boxplot(count~spray)
View(InsectSprays)
Photoperiod <- ordered(spray,levels=c("F","B","C","D","E","A"))
tapply(count,Photoperiod,mean)
tapply(count, spray, mean)
is.factor(spray)
is.factor(Photoperiod)
is.factor(spray)
#oneway test
oneway.test(count~spray)
#aov
aov.out = aov(count ~ spray, data=InsectSprays)
summary(InsectSprays)
summary(aov.out)
#POST-HOC TEST
#tukeyhsd()
TukeyHSD(aov.out)
summary.lm(aov.out)
#homogenity of variance
bartlett.test(count ~ spray, data=InsectSprays)
plot(aov.out)
#non-parametric alternative to anova
kruskal.test(count ~ spray, data=InsectSprays)
#attach ın başka versiyonu
with(PlantGrowth, tapply(weight, group, mean))
with(PlantGrowth, tapply(weight, group, var))
str(PlantGrowth)
with(PlantGrowth, bartlett.test(weight ~ group))
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(PlantGrowth, lm(weight ~ group))
summary(lm.out)
summary.aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(results)
results <- summary.aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(results)
TukeyHSD(lm.out)
TukeyHSD(aov.out)
aov.out <- summary.aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(aov.out)
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(aov.out)
library("httr", lib.loc="~/R/win-library/3.5")
library("XML", lib.loc="~/R/win-library/3.5")
library("dplyr", lib.loc="~/R/win-library/3.5")
library("ggplot2", lib.loc="~/R/win-library/3.5")
library("MASS", lib.loc="~/R/win-library/3.5")
library("MASS", lib.loc="C:/Program Files/R/R-3.5.2/library")
attach(PlantGrowth)
View(PlantGrowth)
with(PlantGrowth, tapply(weight, group, mean))
with(PlantGrowth, tapply(weight, group, var))
with(PlantGrowth, bartlett.test(weight ~ group))
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(PlantGrowth, lm(weight ~ group))
summary(lm.out)
###There is a difference, but where does this difference lie? Post Hoc test:
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(aov.out)
with(PlantGrowth, tapply(weight, group, mean))
with(PlantGrowth, tapply(weight, group, var))
#homogenity of variance #test assumptions
bartlett.test(count ~ spray, data=InsectSprays)
with(PlantGrowth, bartlett.test(weight ~ group))
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(PlantGrowth, lm(weight ~ group))
summary(lm.out)
###There is a difference, but where does this difference lie? Post Hoc test:
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(aov.out)
x <- c(237,289,257,228,303,275,262,304,244,233)
y <- c(194,240,230,186,265,222,242,281,240,212)
d <- c(43,49,27,42,38,53,20,23,4,21)
shapiro.test(d)
x <- c(237,289,257,228,303,275,262,304,244,233)
shapiro.test(x)
shapiro.test(y)
t.test(x,y,alternative="less",paired=TRUE)
t.test(d,alternative="less",paired=TRUE)
t.test(x,y,,alternative="less",paired=TRUE)
t.test(x,y,,alternative="greater",paired=TRUE)
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(PlantGrowth, lm(weight ~ group))
TukeyHSD(lm.out)
drug <- c(x,y)
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(drug, lm(x ~ y))
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(d, lm(x ~ y))
TukeyHSD(d)
drug <- c("x","y")
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(drug, lm(x ~ y))
drug <- data.frame(x,y)
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(drug, lm(x ~ y))
###There is a difference, but where does this difference lie? Post Hoc test:
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(d)
TukeyHSD(drug)
TukeyHSD(lm.out)
TukeyHSD(aov.out)
x <- c(237,289,257,228,303,275,262,304,244,233)
y <- c(194,240,230,186,265,222,242,281,240,212)
d <- c(43,49,27,42,38,53,20,23,4,21)
groups <- c(rep(1,10),rep(2,10))
drug <- data.frame(x,y)
set <- c(x,y,d)
out <- aov(drug~as.factor(groups),data = set)
View(drug)
drug <- data.frame(x,y,d)
with(drug, bartlett.test(x ~ y))
chisq.test(x,y)
t.test(x,y,paired = TRUE,var.equal = TRUE,alternative = c("greater"))
source('C:/Users/user/ucuncu_sinif/sc/final/hypo_test.R', echo=TRUE)
t.test(x,y,paired = TRUE,var.equal = TRUE,alternative = c("two.sided"))
greater
t.test(x,y,paired = TRUE,var.equal = TRUE,alternative = c("greater"))
t.test(x,y,paired = TRUE,var.equal = TRUE,alternative = c("greater"))
t.test(x,y,paired = TRUE,var.equal = TRUE,alternative = c("greater"))
var.test(x,y)
t=read.csv("w.csv")
Location=t$Location
#Finding the count of "NA" values within each feature of the dataset
count= sort(sapply(t, function(y) sum(length(which(is.na(y))))))
#Percentage of "NA" values within each feature of the dataset
countpercent=sapply(t, function(y) round(sum(length(which(is.na(y))))/nrow(t)*100))
#Extracting name of features - having "NA" values more than 1/3 of its number of observations
na_count=which(count>nrow(t)/3)
names=names(na_count)
par(mar=c(5,5.3,4,2))
t=t %>% select(-RISK_MM,-Date,-Location,-names)
#Finding the count of "NA" values within each feature of the dataset.
count2=sapply(t, function(y) sum(length(which(is.na(y)))))
#Percentage of "NA" values within each feature
count2percent=sort(sapply(t, function(y) round(sum(length(which(is.na(y))))/nrow(t)*100)))
t$RainToday <- ifelse(t$RainToday=="Yes", 1, 0)
t$RainToday= factor(t$RainToday)
t$RainTomorrow <- ifelse(t$RainTomorrow=="Yes", 1, 0)
t$RainTomorrow=factor(t$RainTomorrow)
registerDoParallel(cores = 14)
getDoParWorkers()
library(naniar)
library(missRanger)
library(doParallel)
library(ggplot2)
library(tidyverse)
library(onehot)
library(data.table)
library(mltools)
library(caret)
library(car)
library(dplyr)
library(ggplot2)
require(xgboost)
library(DiagrammeR)
library(rpart)
library(rpart.plot)
library(corrplot)
library(VIM)
library(randomForest)
library(ggplot2)
library(graphics)
t=t %>% select(-RISK_MM,-Date,-Location,-names)
#Finding the count of "NA" values within each feature of the dataset.
count2=sapply(t, function(y) sum(length(which(is.na(y)))))
#Percentage of "NA" values within each feature
count2percent=sort(sapply(t, function(y) round(sum(length(which(is.na(y))))/nrow(t)*100)))
t$RainToday <- ifelse(t$RainToday=="Yes", 1, 0)
t$RainToday= factor(t$RainToday)
t$RainTomorrow <- ifelse(t$RainTomorrow=="Yes", 1, 0)
t$RainTomorrow=factor(t$RainTomorrow)
registerDoParallel(cores = 14)
getDoParWorkers()
Weather_Imp_t <- missRanger(t, pmm.k = 3, splitrule = "extratrees", num.trees = 100)
Weather_Imp=Weather_Imp_t
Weather_Imp=cbind(Location,Weather_Imp)
temp_weather_imp=Weather_Imp
names(Weather_Imp)
library(caTools)
set.seed(123)
split = sample.split(Weather_Imp1$RainTomorrow, SplitRatio = 0.75)
training_set = subset(Weather_Imp1, split == TRUE)
test_set = subset(Weather_Imp1, split == FALSE)
names(test_set)
#Scaling the datasets
training_set_sc=training_set[,50:55] %>%
mutate_if(is.numeric, scale)
test_set_sc=test_set[,50:55] %>%
mutate_if(is.numeric, scale)
training_set=cbind(training_set[1:49],training_set_sc,training_set[56:105])
str(training_set)
test_set=cbind(training_set[1:49],training_set_sc,training_set[56:105])
library(caTools)
set.seed(123)
split = sample.split(Weather_Imp1$RainTomorrow, SplitRatio = 0.75)
training_set = subset(Weather_Imp1, split == TRUE)
test_set = subset(Weather_Imp1, split == FALSE)
names(test_set)
#Scaling the datasets
training_set_sc=training_set[,50:55] %>%
mutate_if(is.numeric, scale)
test_set_sc=test_set[,50:55] %>%
mutate_if(is.numeric, scale)
training_set=cbind(training_set[1:49],training_set_sc,training_set[56:105])
str(training_set)
test_set=cbind(training_set[1:49],training_set_sc,training_set[56:105])
data <- read.csv("data.csv")
#POST-HOC TEST
#tukeyhsd()
TukeyHSD(aov.out)
PlantGrowth
###There is a difference, but where does this difference lie? Post Hoc test:
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
# instead of running an ANOVA with aov(), we will run a linear regression with lm()
lm.out = with(PlantGrowth, lm(weight ~ group))
summary(lm.out)
###There is a difference, but where does this difference lie? Post Hoc test:
aov.out <- aov(lm.out) # we can ask for the corresponding ANOVA table
TukeyHSD(lm.out)
getwd()
setwd("..")
setwd("ucuncu_sinif")
setwd("wm")
setwd("project")
setwd("app3")
data <- read.csv("data.csv")
shiny::runApp()
getwd()
setwd("..")
setwd("app4")
runApp()
getwd()
setwd("..")
setwd("app3")
getwd()
runApp()
View(data)
runApp()
getwd()
setwd("..")
setwd("app4")
runApp()
runApp()
setwd("..")
setwd("app3")
runApp()
shiny::runApp()
